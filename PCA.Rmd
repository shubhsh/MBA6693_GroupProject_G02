---
title: "MBA6693 - Group project: Predicting Delinquent Customer"
author: "Group 2"
date: "27 July 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction

CredX needs to analyse the defaulterâ€™s data and categorize defaulters according to the given demographics. In this report we would be cleaning up the datasets, demographics and Credit Bureau and doing univariate and bivariate analysis of the data. Our initial step would be to merge the data set and cleaning it by removing any NA values.  

Our next step would be to do bivariate analysis of all the variables in comparison to performance tag variable 

 

In our analysis we would be taking a 0 performance tag value to be non-defaulters and 1 performance tag value to be defaulters.  

 

The Demographics include variables like age, marital status, education, no of dependents, etc. 

The credit dataset includes variables like no. of times borrowers gave defaulted in last 30, 60 and 90 days.  

 

With this analysis we set to Predict delinquent customers for CredX. This prediction would help CredX acquire right customers using predictive models. The data will help us understand factors affecting credit risk, create strategies to mitigate the acquisition risk and access financial benefits of the project. 

## Importing Libraries

```{r}
library(dplyr)
library(ggplot2)
library(reshape2)
library(Information)
library(lattice)
library(caret)
library(woe)
library(usethis)
library(devtools)
library(groupdata2)
library(fuzzyjoin)
library(scorecard)
install_github("vqv/ggbiplot")
library(ggbiplot)


```

```{r}
library(lares) #install_github("laresbernardo/lares")
```

```{r}
library(ggbiplot)
```


## Readability and Presentation

```{r}
cp_2 <- c("#FEA47F", "#F97F51")
cp_3 <- c("#2A363B", "#E84A5F", "#FF847C")
cp_5 <- c("#2A363B", "#E84A5F", "#FF847C", "#FECEAB", "#99B898")
cp_8 <- c("#FEA47F", "#F97F51", "#B33771", "#3B3B98", "#58B19F", "#BDC581", "#2C3A47", "#82589F")
```


## Data

### Data Loading

```{r}
dem <-read.csv("demogs.csv")
credit <- read.csv("Credit_Bureau.csv")
```

### Data Summary

```{r}
# Summary
summary(dem)
summary(credit)

str(dem)
str(credit)

# No of rows
nrow(dem)
# 71295

nrow(credit)
# 71295
```
## Data Cleaning & Exploratory Data Analysis



### Elimination of Duplicates

```{r}
length(unique(dem$Application.ID)) #demo data set
#71292

length(unique(credit$Application.ID)) #credit data set
#71292

#filtering out the IDs that are duplicated

dem %>%
  group_by(Application.ID) %>%
  filter(n() > 1)
#assign new App ID to the duplicate records or drop them


credit %>%
  group_by(Application.ID) %>%
  filter(n() > 1)


dem <- dem %>%
  group_by(Application.ID) %>%
  filter(n() == 1)

credit <- credit %>%
  group_by(Application.ID) %>%
  filter(n() == 1)

```

### Joining the datasets

```{r}
merged_data <- merge(dem,credit, by=c("Application.ID", "Performance.Tag"))
```

### Renaming the colmuns

```{r}
names(merged_data)[c(1:2, 5:6, 10:29)] <- c("Application_ID", "Performance_Tag", "Marital_Status", "No_Of_Dependents", "Type_Of_Residence", "Months_In_Current_Residence", "Months_In_Current_Company", "No_Of_90_DPD_6_months", "No_Of_60_DPD_6_months", "No_Of_30_DPD_6_months", "No_Of_90_DPD_12_months","No_Of_60_DPD_12_months","No_Of_30_DPD_12_months", "Avg_CC_Utilization_12_months", "Trades_6_months", "Trades_12_months", "PL_Trades_6_months", "PL_Trades_12_months", "Inquiries_6_months", "Inquiries_12_months", "Open_Home_Loan", "Outstanding_Balance", "Total_No_of_trades", "Open_Auto_Loan")

```


### Performance Tag
```{r}
merged_data$Performance_Tag %>%
  is.na() %>%
  sum()

summary(merged_data$Performance_Tag)

merged_data <- merged_data %>%
  filter(!is.na(Performance_Tag))
```

```{r}
# Plot for Performance Tag
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=as.factor(Performance_Tag), y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar() +
  scale_fill_manual(values = cp_2) +
  labs(x="Performance Tag", y="Frequency in 1000s", fill="Performance Tag", title="Frequency of Performance Tag") +
  theme_minimal()
```
```{r}
# Percentage of Default

non_default_count <- as.numeric(table(merged_data$Performance_Tag)[1])
default_count <- as.numeric(table(merged_data$Performance_Tag)[2])

default_percentage <- default_count / (default_count+non_default_count)
default_percentage*100
# 4.22%

```

#### Observation
* Of all the customers, 4.22 percent are defaulters.

### Age

```{r}
#=========#
#   Age   #
#=========#

# Check for Age variable rows with NA values
merged_data$Age %>%
  is.na() %>% sum()
# 0

# Checking for outliers
merged_data$Age %>%
  quantile(seq(0,1, 0.01))

merged_data$Age %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")

## Min age is -3
## Some ages are 0
## Capping minimum age to 18
## Since 18 is the minimum age to avail a credit card

merged_data$Age <- merged_data$Age %>%
  as.numeric()

merged_data[(which(merged_data$Age < 18)), ]$Age <- 18




```

#### Observation

* We see in the age analysis that people in age group of 36-40 are most defaulters. This could be because people from this age group are trying to handle family responsibilities, young children, mid life crisis.  

### Gender

```{r}
#=============#
#   Gender    #
#=============#
# Summary for Gender
merged_data$Gender <- as.factor(merged_data$Gender)
merged_data$Gender %>%
  summary()

# 2 NA's

# Converting NA for Gender variable to "M"
levels(merged_data$Gender)[1] <- "M"

# Plot for frequency of each Gender
ggplot(merged_data, aes(x=Gender, y=..count../1000, fill=Gender)) +
  geom_bar() +
  scale_fill_manual(values = cp_2)+
  labs(x="Gender", y="Frequency in 1000s", fill="Gender", title="Frequency of different Gender") +
  theme_minimal()

# Gender wise Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Gender, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="Gender", y="Frequency in 1000s", fill="Performance Tag", title="Gender wise Performance Tag Frequency")

merged_data$Gender <- unclass(merged_data$Gender)
str(merged_data$Gender) #1 for Male, 2 for Female

```

#### Observation

* Our data set has higher number of males than females in total. There cannot be much difference in default rate in this analysis. The reason number of total males are more than females could be due to male being primary earner of the house.  

### Marital Status

```{r}
#=====================#
#   Marital Status   #
#=====================#

# Summary for Marital status at time of application
merged_data$Marital_Status %>%
  summary()
# 6 NA's

merged_data$Marital_Status <- as.factor(merged_data$Marital_Status)
# Converting NA for Marital status at time of application variable to "Married"
levels(merged_data$Marital_Status)[1] <- "Married"

# Plot for Marital status at time of application frquency
ggplot(merged_data, aes(x=Marital_Status, y=..count../1000, fill=Marital_Status)) +
  geom_bar()+
  scale_fill_manual(values = cp_8)+
  labs(x="Marital Status at time of application", y="Frequency in 1000s", fill="Marital Status", title="Frequency of different Marital Status") +
  theme_minimal()

# Marital Status wise Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Marital_Status, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="Marital Status", y="Frequency in 1000s", fill="Performance Tag", title="Marital Status wise Performance Tag Frequency")

merged_data$Marital_Status <- unclass(merged_data$Marital_Status)
str(merged_data$Marital_Status) #1 for Married, 2 for Single

```

#### Observation

* The percentage of married people taking in higher share of being defaulter can be understandable with the fact that they have family and demands are not just of single person. 

### No of Dependents

```{r}
#=====================#
#   No of Dependents  #
#=====================#

# Checking for NA values
merged_data$No_Of_Dependents <- as.numeric(merged_data$No_Of_Dependents)
merged_data$No_Of_Dependents %>%
  is.na() %>%
  sum()

# 3 NA's

merged_data$No_Of_Dependents[which(is.na(merged_data$No_Of_Dependents))] <- 3

merged_data$No_Of_Dependents %>%
  as.factor() %>%
  summary()

# Checking for outliers
merged_data$No_Of_Dependents %>%
  quantile(seq(0,1,0.01), na.rm = T)

merged_data$No_Of_Dependents %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")

str(merged_data$No_Of_Dependents)


```

### Observation

* Most number of dependent group in our dataset is of 3 people. And this group has taken first position with highest number of defaulters. 

### Income

```{r}
#=============#
#   Income    #
#=============#

# checking for NA values
merged_data$Income %>%
  is.na() %>%
  sum()
# 0

# Checking for outliers
merged_data$Income %>%
  quantile(seq(0,1,0.01), na.rm = T)

merged_data$Income %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")

merged_data$Income %>%
  as.factor() %>%
  levels()

# Converting Income less than 1 to 1.0
merged_data[(which(merged_data$Income < 1)), ] $Income <- 1.0


```

### Observation

* Highest number of defaulters with income variable is in group of 1 - 10. This could be due lack of funds witht them. As the salary increases the default rate falls drastically. The person is growing in career. 

### Education

```{r}
#===============#
#   Education   #
#===============#

# checking for NA values
merged_data$Education <- as.factor(merged_data$Education)
merged_data$Education %>%
  is.na() %>%
  sum()

# 0

# Checking for blank rows
merged_data$Education %>%
  summary()

levels(merged_data$Education)[1] <- "Professional"
attach(merged_data)
#Education <- as.factor(Education)
# Plot for Education Frequency
ggplot(merged_data, aes(x=Education, y=..count../1000, fill=Education)) +
  geom_bar() +
  scale_fill_manual(values=cp_5)+
  labs(x="Education", y="Frequency in 1000s", fill="Education", title="Frequency of Education") +
  theme_minimal()

# Education wise Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Education, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="Education", y="Frequency in 1000s", fill="Performance Tag", title="Education wise Performance Tag Frequency")

merged_data$Education <- unclass(merged_data$Education)
str(merged_data$Education) #1 for Professional, 2 for Bachelor. 3 for Masters, 4 for others and 5 for PhD


```


#### Observation 

* Customers who are pursuing a masters degree are more bound to be defaulters.

### Profession

```{r}
#=================#
#   Profession    #
#=================#

# checking for NA values
merged_data$Profession %>%
  is.na() %>%
  sum()

# 0

# Checking for blank rows
merged_data$Profession %>%
  summary()
merged_data$Profession <- as.factor(merged_data$Profession)
levels(merged_data$Profession)[1] <- "SAL"

# Plot for Profession Frequency
ggplot(merged_data, aes(x=Profession, y=..count../1000, fill=Profession)) +
  geom_bar() +
  scale_fill_manual(values=cp_3)+
  labs(x="Profession", y="Frequency in 1000s", fill="Profession", title="Frequency of Profession") +
  theme_minimal()

# Profession wise Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Profession, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="Profession", y="Frequency in 1000s", fill="Performance Tag", title="Profession wise Performance Tag Frequency")


merged_data$Profession <- unclass(merged_data$Profession)
str(merged_data$Profession) #1 for SAL, 2 for SE, 3 for SE_PROF
```


#### Observation 

* Salaried professionals are more bound to be defaulters.


### Type of Residence

```{r}
#=======================#
#   Type of residence   #
#=======================#
# checking for NA values
merged_data$Type_Of_Residence %>%
  is.na() %>%
  sum()

# 0

# Checking for blank rows
merged_data$Type_Of_Residence %>%
  summary()
merged_data$Type_Of_Residence <- as.factor(merged_data$Type_Of_Residence)
levels(merged_data$Type_Of_Residence)[1] <- "Rented"

# Plot for frequency of type of residence
ggplot(merged_data, aes(x=Type_Of_Residence, y=..count../1000, fill=Type_Of_Residence)) +
  geom_bar() +
  scale_fill_manual(values=cp_5)+
  labs(x="Type of residence", y="Frequency in 1000s", fill="Type of residence", title="Frequency of Type of residence") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=40, hjust=1))

# Type of Residence wise Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Type_Of_Residence, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = cp_2) +
  labs(x="Type of Residence", y="Frequency in 1000s", fill="Performance Tag", title="Type of Residence wise Performance Tag Frequency") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=40, hjust=1))



merged_data$Type_Of_Residence <- unclass(merged_data$Type_Of_Residence)
str(merged_data$Type_Of_Residence) #chr [1:5] "Rented" "Company provided" "Living with Parents" "Others", Owned

```

#### Observation 

* People who live in a rented apartment have a higher defaulter rate. This is also due to the fact that most of the customers are the ones who live in a rented apartment




### No of Months in Current Residence

```{r}
#===========================================#
#   Number of months in current residence   #
#===========================================#

# Checking for NA values
merged_data$Months_In_Current_Residence %>%
  is.na() %>%
  sum()

# 0

# Checking for outliers
merged_data$Months_In_Current_Residence %>%
  quantile(seq(0,1,0.01), na.rm = T)

merged_data$Months_In_Current_Residence %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")





```


#### Observation

* Most of the customers are the ones who live in a place they moved in within the last year. Therefore, most number of defaulters are from this group. 


### Number of Months in Current Company

```{r}
#=========================================#
#   Number of months in current company   #
#=========================================#

# Checking for NA values
merged_data$Months_In_Current_Company %>%
  is.na() %>%
  sum()

# 0

# Checking for outliers
merged_data$Months_In_Current_Company %>%
  quantile(seq(0,1,0.01), na.rm = T)


merged_data$Months_In_Current_Company %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")

# Capping No of months in current company to 74
merged_data[(which(merged_data$Months_In_Current_Company > 74)),] $Months_In_Current_Company <- 74



```



#### Observation

* Career stability also affects the performance tag. People who have stayed with their current employer for less than a year have the maximum representation in defaulters' list. And people who have stayed in a company for more than 6 years are most stable and hence the least number of defaulters from this group.


### No of times 90 DPD or worse in last 6 months

```{r}
#===================================================#
#   No of times 90 DPD or worse in last 6 months    #
#===================================================#

# Checking for NA values
merged_data$No_Of_90_DPD_6_months %>%
  is.na() %>%
  sum()

# 0

merged_data$No_Of_90_DPD_6_months %>%
  as.factor() %>%
  summary()



default_percentage <- default_count / (default_count+non_default_count)


no_def <- table(merged_data$No_Of_90_DPD_6_months[(which(merged_data$Performance_Tag == 0))])
no_def
def <-  table(merged_data$No_Of_90_DPD_6_months[(which(merged_data$Performance_Tag != 0))])
def
def/(def+no_def)*100

# No of times 90 DPD or worse in last 6 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=No_Of_90_DPD_6_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of times 90 DPD or worse in last 6 months", y="Frequency in 1000s", fill="Performance Tag", title="No of times 90 DPD or worse in last 6 months vs Performance Tag Frequency")

```



#### Observation

* People who have had 3 90 DPD or worse without payment in last six months are most likely to be the defaulters since their percentage of default is highest at 11.06.


### No of times 60 DPD or worse in last 6 months
```{r}
#===================================================#
#   No of times 60 DPD or worse in last 6 months    #
#===================================================#

# Checking for NA values
merged_data$No_Of_60_DPD_6_months %>%
  is.na() %>%
  sum()

# 0

merged_data$No_Of_60_DPD_6_months %>%
  as.factor() %>%
  summary()

no_def <- table(merged_data$No_Of_60_DPD_6_months[(which(merged_data$Performance_Tag == 0))])
no_def
def <-  table(merged_data$No_Of_60_DPD_6_months[(which(merged_data$Performance_Tag != 0))])
def
def/(def+no_def)*100

# No of times 60 DPD or worse in last 6 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=No_Of_60_DPD_6_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of times 60 DPD or worse in last 6 months", y="Frequency in 1000s", fill="Performance Tag", title="No of times 60 DPD or worse in last 6 months vs Performance Tag Frequency")
```


#### Observation

* People who have had 3 60 DPD or worse without payment in last six months are most likely to be the defaulters since their percentage of default is highest at 10.07.


### No of times 30 DPD or worse in last 6 months
```{r}
#===================================================#
#   No of times 30 DPD or worse in last 6 months    #
#===================================================#

# Checking for NA values
merged_data$No_Of_30_DPD_6_months %>%
  is.na() %>%
  sum()

# 0

merged_data$No_Of_30_DPD_6_months %>%
  as.factor() %>%
  summary()

no_def <- table(merged_data$No_Of_30_DPD_6_months[(which(merged_data$Performance_Tag == 0))])
no_def
def <-  table(merged_data$No_Of_30_DPD_6_months[(which(merged_data$Performance_Tag != 0))])
def
def/(def+no_def)*100


# No of times 30 DPD or worse in last 6 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=No_Of_30_DPD_6_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of times 30 DPD or worse in last 6 months", y="Frequency in 1000s", fill="Performance Tag", title="No of times 30 DPD or worse in last 6 months vs Performance Tag Frequency") 
```



#### Observation

* People who have had 5 30 DPD or worse without payment in last six months are most likely to be the defaulters since their percentage of default is highest at 11.14.



### No of times 90 DPD or worse in last 12 months
```{r}
#===================================================#
#   No of times 90 DPD or worse in last 12 months   #
#===================================================#

# Checking for NA values
merged_data$No_Of_90_DPD_12_months %>%
  is.na() %>%
  sum()

# 0

merged_data$No_Of_90_DPD_12_months %>%
  as.factor() %>%
  summary()

no_def <- table(merged_data$No_Of_90_DPD_12_months[(which(merged_data$Performance_Tag == 0))])
no_def
def <-  table(merged_data$No_Of_90_DPD_12_months[(which(merged_data$Performance_Tag != 0))])
def
def/(def+no_def)*100


# No of times 90 DPD or worse in last 12 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=No_Of_90_DPD_12_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of times 90 DPD or worse in last 12 months", y="Frequency in 1000s", fill="Performance Tag", title="No of times 90 DPD or worse in last 12 months vs Performance Tag Frequency") 
```

### No of times 60 DPD or worse in last 12 months
```{r}
#===================================================#
#   No of times 60 DPD or worse in last 12 months   #
#===================================================#

# Checking for NA values
merged_data$No_Of_60_DPD_12_months %>%
  is.na() %>%
  sum()

# 0

merged_data$No_Of_60_DPD_12_months %>%
  as.factor() %>%
  summary()


# No of times 60 DPD or worse in last 12 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=No_Of_60_DPD_12_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of times 60 DPD or worse in last 12 months", y="Frequency in 1000s", fill="Performance Tag", title="No of times 60 DPD or worse in last 12 months vs Performance Tag Frequency")
```

### No of times 30 DPD or worse in last 12 months
```{r}
#===================================================#
#   No of times 30 DPD or worse in last 12 months   #
#===================================================#

# Checking for NA values
merged_data$No_Of_30_DPD_12_months %>%
  is.na() %>%
  sum()
# 0

merged_data$No_Of_30_DPD_12_months %>%
  as.factor() %>%
  summary()


# No of times 30 DPD or worse in last 12 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=No_Of_30_DPD_12_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of times 30 DPD or worse in last 12 months", y="Frequency in 1000s", fill="Performance Tag", title="No of times 30 DPD or worse in last 12 months vs Performance Tag Frequency") 

```

### Correlation of DPD Variables
```{r}
#===================================#
#   Correlation of DPD Variables    #
#===================================#

DPD_data_6 <- merged_data[, c(13:15)]
DPD_data_12 <- merged_data[, c(16:18)]

cor_DPD_6 <- round(cor(DPD_data_6), 2)
cor_DPD_6
melted_cor_DPD_6 <- melt(cor_DPD_6)

cor_DPD_12 <- round(cor(DPD_data_12), 2)
melted_cor_DPD_12 <- melt(cor_DPD_12)

# DPD Correlation heat map for 6 months
ggplot(melted_cor_DPD_6, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  labs(x="", y="", title="DPD 6 months Heat Map", fill="Value") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=40, hjust=1))

# DPD Correlation heat map for 12 months
ggplot(melted_cor_DPD_12, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  labs(x="", y="", title="DPD 12 months Heat Map", fill="Value") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=40, hjust=1))
```

### Average Credit Card utilisation in last 12 months

```{r}
#=======================================================#
#   Average Credit Card utilization in last 12 months   #
#=======================================================#

# Checking for NA values
merged_data$Avg_CC_Utilization_12_months %>%
  is.na() %>%
  sum()
# 1058

merged_data$Avg_CC_Utilization_12_months %>%
  summary()

# Replacing the NA value with the median
merged_data$Avg_CC_Utilization_12_months[which(is.na(merged_data$Avg_CC_Utilization_12_months))] <- 15


# Checking for outliers
merged_data$Avg_CC_Utilization_12_months %>%
  quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$Avg_CC_Utilization_12_months %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")

merged_data[(which(merged_data$Avg_CC_Utilization_12_months > 103)),] $Avg_CC_Utilization_12_months <- 103



```

### No of trades opened in last 6 months
```{r}

#==========================================#
#   No of trades opened in last 6 months   #
#==========================================#

# Checking for NA values
merged_data$Trades_6_months %>%
  is.na() %>%
  sum()

# 1

merged_data$Trades_6_months %>%
  summary()

# Replacing the NA value with the median
merged_data$Trades_6_months[which(is.na(merged_data$Trades_6_months))] <- 2

# Checking for outliers
merged_data$Trades_6_months %>% quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$Trades_6_months %>% boxplot(border = "#6fa058", outcol = "#ee853f")

merged_data[(which(merged_data$Trades_6_months > 6)),] $Trades_6_months <- 6

# No of trades opened in last 6 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Trades_6_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of trades opened in last 6 months", y="Frequency in 1000s", fill="Performance Tag", title="No of trades opened in last 6 months vs Performance Tag Frequency") 

```

### No of trades opened in last 12 months
```{r}
#===========================================#
#   No of trades opened in last 12 months   #
#===========================================#

# Checking for NA values
merged_data$Trades_12_months %>%
  is.na() %>%
  sum()

# 0

# Checking for outliers
merged_data$Trades_12_months %>% quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$Trades_12_months %>% boxplot(border = "#6fa058", outcol = "#ee853f")

merged_data[(which(merged_data$Trades_12_months > 19)),] $Trades_12_months <- 19

# No of trades opened in last 12 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Trades_6_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of trades opened in last 12 months", y="Frequency in 1000s", fill="Performance Tag", title="No of trades opened in last 12 months vs Performance Tag Frequency") 

```

### Correlation of trades opened

```{r}
#===================================#
#   Correlation of trades opened    #
#===================================#

trades_opened <- merged_data[, c(20, 21)]

cor_trades_opened <- round(cor(trades_opened), 2)
melted_cor_trades_opened <- melt(cor_trades_opened)

# DPD Correlation heat map for 6 months
ggplot(melted_cor_trades_opened, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  labs(x="", y="", title="Trades Opened Heat Map", fill="Value") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=40, hjust=1))
```




### No of PL trades opened in last 6 months

```{r}
#==============================================#
#   No of PL trades opened in last 6 months    #
#==============================================#

# Checking for NA values
merged_data$PL_Trades_6_months  %>%
  is.na() %>%
  sum()

# 0

# Checking for outliers
merged_data$PL_Trades_6_months  %>% quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$PL_Trades_6_months  %>% boxplot(border = "#6fa058", outcol = "#ee853f")

merged_data[(which(merged_data$PL_Trades_6_months  > 5)),] $PL_Trades_6_months  <- 5

# No of PL trades opened in last 6 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=PL_Trades_6_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of PL trades opened in last 6 months", y="Frequency in 1000s", fill="Performance Tag", title="No of PL trades opened in last 6 months vs Performance Tag Frequency")
```

### No of PL trades opened in last 12 months

```{r}
#===============================================#
#   No of PL trades opened in last 12 months    #
#===============================================#

# Checking for NA values
merged_data$PL_Trades_12_months  %>%
  is.na() %>%
  sum()

# 0

# Checking for outliers
merged_data$PL_Trades_12_months  %>% quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$PL_Trades_12_months  %>% boxplot(border = "#6fa058", outcol = "#ee853f")

#merged_data[(which(merged_data$PL_Trades_12_months  > 10)),] $PL_Trades_12_months  <- 10

# No of PL trades opened in last 12 months vs Performance Tag Frequency
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=PL_Trades_12_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of PL trades opened in last 12 months", y="Frequency in 1000s", fill="Performance Tag", title="No of PL trades opened in last 12 months vs Performance Tag Frequency") 
```

### Correlation of PL trades opened

```{r}
#===================================#
#   Correlation of PL trades opened    #
#===================================#

pl_trades_opened <- merged_data[, c(22, 23)]

cor_pl_trades_opened <- round(cor(pl_trades_opened), 2)
melted_cor_pl_trades_opened <- melt(cor_pl_trades_opened)

# DPD Correlation heat map for 6 months
ggplot(melted_cor_pl_trades_opened, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  labs(x="", y="", title="PL Trades Opened Heat Map", fill="Value") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=40, hjust=1))

```


### No of inquiries in last 6 months excluding home auto loan

```{r}
#===============================================================#
#   No of inquiries in last 6 months excluding home auto loan   #
#===============================================================#

# Checking for NA values
merged_data$Inquiries_6_months %>%
  is.na() %>%
  sum()

# 0

# Checking for outliers
merged_data$Inquiries_6_months %>%
  quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$Inquiries_6_months %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")

merged_data[(which(merged_data$Inquiries_6_months > 7)),] $Inquiries_6_months <- 7

# No of inquiries in last 6 months excluding home auto loan
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Inquiries_6_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of inquiries in last 6 months excluding home auto loan", fill="Performance Tag", title="No of inquiries in last 6 months excluding home auto loan") 
```

### No of inquiries in last 12 months excluding home auto loan

```{r}
#=================================================================#
#   No of inquiries in last 12 months excluding home auto loan    #
#=================================================================#

# Checking for NA values
merged_data$Inquiries_12_months %>%
  is.na() %>%
  sum()

# 0

# Checking for outliers
merged_data$Inquiries_12_months %>%
  quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$Inquiries_12_months %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")

merged_data[(which(merged_data$Inquiries_12_months > 12)),] $Inquiries_12_months <- 12

# No of inquiries in last 6 months excluding home auto loan
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Inquiries_12_months, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="No of inquiries in last 12 months excluding home auto loan", fill="Performance Tag", title="No of inquiries in last 12 months excluding home auto loan") 

```

### Presence of open home loan

```{r}
#=================================#
#   Presence of open home loan    #
#=================================#

# Checking for NA values
merged_data$Open_Home_Loan %>%
  is.na() %>%
  sum()

# 272

merged_data$Open_Home_Loan %>%
  as.factor() %>%
  summary()

merged_data$Open_Home_Loan[which(is.na(merged_data$Open_Home_Loan))] <- 0


```

### Presence of open auto loan

```{r}
#=================================#
#   Presence of open auto loan    #
#=================================#

# Checking for NA values
merged_data$Open_Auto_Loan %>%
  is.na() %>%
  sum()

# 0

merged_data$Open_Auto_Loan %>%
  as.factor() %>%
  summary()



```

### Outstanding Balance

```{r}
#=========================#
#   Outstanding Balance   #
#=========================#

# Checking for NA values
merged_data$Outstanding_Balance %>%
  is.na() %>%
  sum()
# 272

merged_data$Outstanding_Balance %>%
  summary()
# Median = 774985

merged_data$Outstanding_Balance[which(is.na(merged_data$Outstanding_Balance))] <- 774985


# Checking for outliers
merged_data$Outstanding_Balance %>%
  quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$Outstanding_Balance %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")


```

### Total no of trades

```{r}
#=========================#
#   Total no of trades    #
#=========================#

# Checking for NA values
merged_data$Total_No_of_trades %>%
  is.na() %>%
  sum()

# Checking for outliers
merged_data$Total_No_of_trades %>%
  quantile(seq(0, 1, 0.01), na.rm = T)

merged_data$Total_No_of_trades %>%
  boxplot(border = "#6fa058", outcol = "#ee853f")

merged_data[(which(merged_data$Total_No_of_trades > 20)),] $Total_No_of_trades <- 20

# Total no of trades
merged_data %>%
  filter(!is.na(Performance_Tag)) %>%
  ggplot(aes(x=Total_No_of_trades, y=..count../1000, fill=as.factor(Performance_Tag))) +
  geom_bar() +
  theme_minimal()+
  scale_fill_manual(values = cp_2) +
  labs(x="Total no of trades", fill="Performance Tag", title="Total no of trades vs Performance Tag") 

```


### Dataset for PCA




Removing the column Application ID

```{r}
merged_data_pca <- merged_data[,-c(1)]
```

```{r}
merged_data_pca <- as.data.frame(sapply(merged_data_pca, as.numeric))
str(merged_data_pca)
```



```{r}

#Splitting data into train and test
library(caTools)
library(ROSE)
set.seed(0) 
split = sample.split(merged_data_pca$Performance_Tag, SplitRatio = 0.99) 
  
training_set = subset(merged_data_pca, split == TRUE) 
test_set = subset(merged_data_pca, split == FALSE) 

training_set_SMOTE <- ROSE(Performance_Tag ~ ., training_set,seed=1)$data
table(training_set_SMOTE$Performance_Tag)
```

```{r}
merged_data_pca_predictors <- training_set[,-c(1)]
class_memberships <- as.factor(training_set[,c(1)])
merged_data_pca_upsample <- upSample(merged_data_pca_predictors, class_memberships, list = FALSE, yname = "Performance_Tag")
```

```{r}
set.seed(1234)
#merged_data_pca_subset <- merged_data_pca %>% sample_frac(1, replace = FALSE) #use frac as 0.0009 for plot
names(training_set_SMOTE)[c(1:28)] <- c("y","x_1","x_2","x_3","x_4","x_5","x_6","x_7","x_8","x_9","x_10","x_11","x_12","x_13","x_14","x_15","x_16","x_17","x_18","x_19","x_20","x_21","x_22","x_23","x_24","x_25","x_26","x_27")

names(test_set)[c(1:28)] <- c("y","x_1","x_2","x_3","x_4","x_5","x_6","x_7","x_8","x_9","x_10","x_11","x_12","x_13","x_14","x_15","x_16","x_17","x_18","x_19","x_20","x_21","x_22","x_23","x_24","x_25","x_26","x_27")
#names(merged_data_pca_upsample)[c(1:28)] <- c("x_1","x_2","x_3","x_4","x_5","x_6","x_7","x_8","x_9","x_10","x_11","x_12","x_13","x_14","x_15","x_16","x_17","x_18","x_19","x_20","x_21","x_22","x_23","x_24","x_25","x_26","x_27", "y")
#merged_data_pca_upsample

```


```{r}
merged_data_pca_original <- prcomp(merged_data_pca[,-c(1)], center = TRUE,scale. = TRUE)
merged_data.pca_test_set <- prcomp(test_set[,-c(1)], center = TRUE,scale. = TRUE)
merged_data.pca <- prcomp(training_set_SMOTE[,-c(1)], center = TRUE,scale. = TRUE)
summary(merged_data.pca)


std_dev <- merged_data.pca$sdev
prop_varex <- std_dev/sum(std_dev)
sum(prop_varex[1:27])
```


```{r}
plot(cumsum(prop_varex), xlab = "Principal Component",ylab = "Cumulative Proportion of Variance Explained",type = "b")
abline(h=0.975,col='red',v=27)
```
```{r}
ggbiplot(merged_data.pca)
```

```{r}
ggscreeplot(merged_data.pca)
```

```{r}
plot(cumsum(prop_varex), xlab = "Principal Component",ylab = "Cumulative Proportion of Variance Explained",type = "b")
abline(h=0.975,col='red',v=27)
```

```{r}
merged_data.pca$x
```



```{r}
#dim(merged_data_pca_subset$y)
training_upsampled_pca_rf <-data.frame(Performance_Tag=as.factor(training_set_SMOTE$y), merged_data.pca$x)
#training_data_rf <- training_data_rf[,1:28]

metric <- "Accuracy"
control <- trainControl(method="cv", number=10)
mtry <- sqrt(ncol(training_set))
tunegrid <- expand.grid(.mtry=mtry)
```

```{r}
model_rf <- train(Performance_Tag~.,data=training_upsampled_pca_rf, method="rf", tuneGrid=tunegrid, trControl=control)
```







```{r}
print(model_rf)
summary(model_rf)
```
### Preparing test set

```{r}
test_set_pca <- data.frame(Performance_Tag=as.factor(test_set$y), merged_data.pca_test_set$x)
test_original_pca_rf <- data.frame(Performance_Tag=as.factor(merged_data_pca$Performance_Tag), merged_data_pca_original$x)
```


```{r}
prediction_rf = predict(model_rf, newdata =test_set_pca[-1])
confusionMatrix(prediction_rf,as.factor(test_set$y))
```

```{r}
test_set_svm$Performance_Tag <- test_set_pca$Performance_Tag

```

```{r}
mplot_density(test_set_pca$Performance_Tag, prediction_rf)
mplot_roc(test_set_pca$Performance_Tag, prediction_rf)
mplot_cuts(as.numeric(prediction_rf))
mplot_splits(test_set_pca$Performance_Tag, as.numeric(prediction_rf))
mplot_gain(test_set_pca$Performance_Tag,as.numeric(prediction_rf), target=1)
mplot_response(test_set_pca$Performance_Tag,as.numeric(prediction_rf), target=1)
```

```{r}
prediction_rf = predict(model_rf, newdata = test_set[-1])
#cm = table(test_set$Performance_Tag, prediction_rf) 
#cm
length(prediction_rf)
length(test_original_pca_rf$Performance_Tag)
confusionMatrix(prediction_rf,test_set$y)
```



```{r}
library(ROCR)
rocs <- performance(prediction_rf, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Bagged Trees", "Random Forest", "GBM"),
       fill = 1:m)

```

